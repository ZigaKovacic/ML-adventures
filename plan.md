### Naloga 1: Sentiment analysis
Sentiment analysis z uporabo modelov iz knjižnice scikit-learn.
Lahko dobesedno slediš notebooku iz gradiva.

Gradivo:
* [notebook](https://sites.pitt.edu/~naraehan/presentation/Movie+Reviews+sentiment+analysis+with+Scikit-Learn.html)
* [Working with text data](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)

### Naloga 2: Sentiment analysis s knjižnico Transformers
Koraki so opisani [tukaj](https://huggingface.co/transformers/training.html).

Ključno je, da implementiraš poglavji:
1. Fine-tuning in PyTorch with the Trainer API
2. Fine-tuning in native PyTorch

Gradivo:
* Knjižnica Transformers
    * [članek](https://arxiv.org/pdf/1910.03771.pdf)
    * [repozitorij](https://github.com/huggingface/transformers)
* PyTorch
    * [examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)

### Naloga 3: Uvoz podatkov TOPv2
Uvozi TOPv2 podatke in sledi pripravi, opisani v članku.

Gradivo:
* [Low-Resource Domain Adaptation for
Compositional Task-Oriented Semantic Parsing](https://arxiv.org/pdf/2010.03546.pdf)

### Naloga 4: Pointer networks
Seznani se s konceptom Pointer networks oziroma pointer-generator networks, ki je relevanten
za implementacijo članka. Ključno je, da iz člankov iz gradiva (+ drugi viri, recimo blog posti) razumeš koncept in dobiš približno idejo, kako bi to implementiral v Pytorch.

Gradivo:
* [Pointer networks](https://arxiv.org/abs/1506.03134)
* [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)
* [Don't parse, generate!](https://arxiv.org/abs/2001.11458)

* [Pointer networks, attention, ...](https://towardsdatascience.com/understanding-pointer-networks-81fbbc1ddbc8)
* [Pointer networks with transformers](https://towardsdatascience.com/pointer-networks-with-transformers-1a01d83f7543) + [Colab notebook z implementacijo za convex hull](https://colab.research.google.com/drive/1lobspU9b7dTO_HuoX-3nibZspTwfa5aX?usp=sharing)

Implementacija za rešitev convex hull problema uporabi Transformer za encoder in decoder, glej članek:
* [Attention is all you need](https://arxiv.org/abs/1706.03762)

### Nekaj člankov za prebrat
* [NLP (almost) from scratch](https://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)
* [Word2Vec](https://arxiv.org/abs/1301.3781)
* [Attention is all you need](https://arxiv.org/abs/1706.03762)
* [BERT paper](https://arxiv.org/abs/1810.04805)
* [T5 paper](https://arxiv.org/abs/1910.10683)
