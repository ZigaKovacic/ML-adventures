{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f18dc7de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/zileto/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc8767b",
   "metadata": {},
   "source": [
    "data represents the features, which are the variables that help the model learn how to predict. target includes the actual labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "899a2699",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_dir =  r'/Users/Zileto/ipython-in-depth/notebooks/movie_reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "484edec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here you load all the .txt files from movie_reviews to m_train\n",
    "m_train = load_files(m_dir, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "42de05e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(m_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6abf2587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neg', 'pos']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .target_names outputs a list of classes in the training set \n",
    "# m_train, in this case negative & positive\n",
    "m_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c9eb3243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b\"arnold schwarzenegger has been an icon for action enthusiasts , since the late 80's , but lately his films have been very sloppy and the one-liners are getting worse . \\nit's hard seeing arnold as mr . freeze in batman and robin , especially when he says tons of ice jokes , but hey he got 15 million , what's it matter to him ? \\nonce again arnold has signed to do another expensive blockbuster , that can't compare with the likes of the terminator series , true lies and even eraser . \\nin this so cal\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#You can access the content of a particular file in the\n",
    "#training set as follows:\n",
    "\n",
    "m_train.data[0][:500]\n",
    "\n",
    "#[:500] outputs only the first 500 characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "08854383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Zileto/ipython-in-depth/notebooks/movie_reviews/neg/cv405_21868.txt'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train.filenames[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5e1b1d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#target is the variable you want to predict, i.e. 'neg'=0 \n",
    "#or 'pos'=1\n",
    "m_train.target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fac1d5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_train.target[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9358c8f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "51b39815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize m_vector object, a CoutVectorizer to \n",
    "#use NLTK's tokenizer instead of \n",
    "# its default one (which ignores punctuation and stopwords). \n",
    "# Minimum document frequency set to 1. \n",
    "m_vec = CountVectorizer(min_df=2, tokenizer=nltk.word_tokenize)\n",
    "\n",
    "# turn movie train data into a vector - \n",
    "#sparse vector of word frequency counts\n",
    "m_counts = m_vec.fit_transform(m_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1c5c2543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 25280)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_counts.shape \n",
    "# dimension of 2000 (document count)\n",
    "# by 25280(# of unique wordssents_counts.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "35331b99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "411"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_vec.vocabulary_.get('?') #which index is assigned to '?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "161be904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19657"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_vec.vocabulary_.get('seagal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "9b62dca2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 2, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_counts.toarray()\n",
    "# analysing the first line (document 1) of the array: \n",
    "# - there is 0 tokens with index 0, there are 2 \n",
    "# tokens with index 2 etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af92b73",
   "metadata": {},
   "source": [
    "TF-IDF values: converts raw frequency counnts (occurences) of tokens to TF-IDF (Term Frequency -- Inverse Document Frequency) values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ecddc9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "m_tfidf = tfidf_transformer.fit_transform(m_counts)\n",
    "# raw counts have been normalized against document length\n",
    "# terms that are found across many docs are weighted down\n",
    "\n",
    "# fit(..) method to fit our estimator to the data\n",
    "#transform(..) method to transform our count-matrix \n",
    "#to a tf-idf representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3210b880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.03844965, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "8e4c8e22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 25280)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_tfidf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bc11e8",
   "metadata": {},
   "source": [
    "Training and testing a Naive Bayes classifier\n",
    "\n",
    "In this section I will build a classifier using MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "68392800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First I split the data 'movies' into trainig and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "docs_train, docs_test, y_train, y_test =train_test_split(m_tfidf,\n",
    "m_train.target, test_size=0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1011a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here I will train a Multimoda Niave Bayes classifier\"\n",
    "clf = MultinomialNB().fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d5d25ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results, find accuracy by comparing \n",
    "# the predicted and test value\n",
    "y_pred = clf.predict(docs_test)\n",
    "sklearn.metrics.accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "19401e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "#cm = confusion_matrix(y_test, y_pred)\n",
    "#cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34736f71",
   "metadata": {},
   "source": [
    "Testing on a random set 'reviews_new'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "5e47a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reviews_new = ['This movie was excellent', 'Absolute joy ride', \n",
    "            'Steven Seagal was terrible', 'Steven Seagal shined through.', \n",
    "              'This was certainly a movie', 'Two thumbs up', 'I fell asleep halfway through', \n",
    "              \"We can't wait for the sequel!!\", '!', '?', 'I cannot recommend this highly enough', \n",
    "              'instant classic.', 'Steven Seagal was amazing. His performance was Oscar-worthy.']\n",
    "reviews_new_counts = m_vec.transform(reviews_new)\n",
    "reviews_new_tfidf = tfidf_transformer.transform(reviews_new_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "fbf09f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have classifier make a prediction\n",
    "predicted = clf.predict(reviews_new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "acaf0a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'This movie was excellent'  >>  pos\n",
      "'Absolute joy ride'  >>  pos\n",
      "'Steven Seagal was terrible'  >>  neg\n",
      "'Steven Seagal shined through.'  >>  neg\n",
      "'This was certainly a movie'  >>  neg\n",
      "'Two thumbs up'  >>  neg\n",
      "'I fell asleep halfway through'  >>  neg\n",
      "\"We can't wait for the sequel!!\"  >>  neg\n",
      "'!'  >>  neg\n",
      "'?'  >>  neg\n",
      "'I cannot recommend this highly enough'  >>  pos\n",
      "'instant classic.'  >>  pos\n",
      "'Steven Seagal was amazing. His performance was Oscar-worthy.'  >>  neg\n"
     ]
    }
   ],
   "source": [
    "for review, category in zip(reviews_new, pred):\n",
    "    print('%r  >>  %s' % (review, m_train.target_names[category]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95fccaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
