{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task-2-Part-1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfIJgQjBSgK1"
      },
      "source": [
        "Summary:\n",
        "\n",
        "1. prepare datasets:\n",
        "\t- transformers datasets\n",
        "\t- load_datasets\n",
        "2. tokenize raw_dataset\n",
        "\t- transformers AutoTokenizer.from_pretrained (bert-base-cased)\n",
        "\t- define tokenize map function\n",
        "\t- declair test, eval data\n",
        "3. model\n",
        "\t- AutoModelForSequenceClassification (bert-base-cased)\n",
        "\t- training arguments using TrainingArguments from trainsformers\n",
        "\t- using numpy define compute_metrics function for evaluation of our model\n",
        "\t- create a \"trainer\" = Trainer with specific model, argument, tran&eval data\n",
        "\n",
        "\n",
        "Finally run trainer.train() to train/fine-tune our model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kKdCSGZR0pGn"
      },
      "source": [
        "#preparing datasets\n",
        "!pip install transformers datasets\n",
        "\n",
        "\n",
        "from datasets import load_dataset, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "#The raw_datasets object is a dictionary with three keys:\n",
        "# \"train\", \"test\" and \"unsupervised\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LyngAjTF18E_"
      },
      "source": [
        "#for our purpuse we will use BERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tk_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation = True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tk_function, batched=True)\n",
        "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "full_train_dataset = tokenized_datasets[\"train\"]\n",
        "full_eval_dataset = tokenized_datasets[\"test\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keui3k74Ssof"
      },
      "source": [
        "#Fine-tuning in PyTorch with the Trainer API\n",
        "\n",
        "#define our model from pretrained Transformer models\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels = 2)\n",
        "\n",
        "#instantiate TrainingArguments/hyperparameters\n",
        "#evaluation_strategy=\"epoch\" enables regular evaluations of 'trainer'\n",
        "t_args = TrainingArguments(\"test_trainer\", evaluation_strategy=\"epoch\")\n",
        "\n",
        "# since there is not evaluation of how our model preformed by default in our \"Trainer\",\n",
        "# we are going to manually compute these matrics\n",
        "\n",
        "!pip install np\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "\n",
        "#metric = load_metric(\"accuracy\")\n",
        "\n",
        "#def compute_metrics(eval_pred):\n",
        "#  logits, labels = eval_pred\n",
        "#  predictions = np.argmax(logits, axis=-1)\n",
        "#  return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "#create a trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=t_args, \n",
        "    train_dataset=small_train_dataset, \n",
        "    eval_dataset=small_eval_dataset,\n",
        "    #compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "#to fine-tune our model, we just call the following command using our small_trainig data\n",
        "trainer.train()\n",
        "trainer.evaluate()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
