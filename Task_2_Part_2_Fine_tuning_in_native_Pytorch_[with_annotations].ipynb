{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Task-2-Part-2-Fine-tuning in native Pytorch [with annotations].ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1ZF9JoCTniW"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "#preparing datasets\n",
        "!pip install transformers datasets\n",
        "#!conda install pytorch==1.7.0 torchvision==0.8.1 torchaudio==0.7.0 -c pytorch\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset,  load_metric\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
        "\n",
        "raw_datasets = load_dataset(\"imdb\")\n",
        "\n",
        "#The raw_datasets object is a dictionary with three keys:\n",
        "# \"train\", \"test\" and \"unsupervised\"\n",
        "\n",
        "#for our purpuse we will use BERT dictionary/model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "def tk_function(examples):\n",
        "  return tokenizer(examples[\"text\"], padding=\"max_length\", truncation = True)\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(tk_function, batched=True)\n",
        "\n",
        "#before using pytorch, we need to modify our tokenized datasets according to Pytorch requirements\n",
        "#1. we remove the column \"text\" (column corresponding to values the model does not expect) - we only keep the tokenized values\n",
        "#2. we rename the \"label\" column to \"labels\"\n",
        "#3. change the format of the datasets from lists to TENSORS\n",
        "\n",
        "torch.cuda.empyt_cache()\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGaFyMhYHe-a"
      },
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "  tokenized_datasets[\"train\"], shuffle=True, batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "eval_dataloader = DataLoader(\n",
        "  tokenized_datasets[\"test\"], batch_size=8, collate_fn=data_collator\n",
        ")\n",
        "\n",
        "#DISCLAIMER !!!! - I did not use the below code from the tutorial, because it reporeted different errors ('label', ...)\n",
        "#Instead I used the above way of declaring train & eval dataloaders\n",
        "#This is the code i did NOT use (as mentioned):\n",
        "  #small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
        "  #small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))\n",
        "  #now we can define our dataloader class. It represents a Python iterable over a dataset, \n",
        "  #with support for map-style and iterable-style datasets, customizing data loading order,\n",
        "  #automatic batching, single- and multi-process data loading, automatic memory pinning.\n",
        "  #train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
        "  #eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tnYLD4qDHlJw"
      },
      "source": [
        "# again we define our model\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)\n",
        "\n",
        "# now the only two things missing are an optimizer and a learning rate scheduler\n",
        "# AdamW is a default optimizer used by Traniner, where lr is the learning rate\n",
        "\n",
        "from transformers import AdamW\n",
        "from tranformers import get_scheduler\n",
        "\n",
        "# to decrease trainig time  use a bigger lr value (=5e-2)\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "# num_epochs indicates how many times the .train will use all of the training data\n",
        "num_epochs = 3\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    \"linear\",\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=num_training_steps #defined above\n",
        ")\n",
        "\n",
        "\n",
        "#to use our GPU to train our model we define a device as follows:\n",
        "\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RrvQen4XHpW2"
      },
      "source": [
        "# we can use tqdm library to display a progress bar\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDVwibiZjXfW"
      },
      "source": [
        "#Training the model + explanation\n",
        "\n",
        "#loss.backwards():\n",
        "    #loss.backward() computes dloss/dx for every parameter x which has requires_grad=True. These are accumulated into x.grad for every parameter x. In pseudo-code:\n",
        "    #x.grad += dloss/dx\n",
        "#optimizer.step()\n",
        "    #optimizer.step updates the value of x using the gradient x.grad. For example, the SGD optimizer performs:\n",
        "    #x += -lr * x.grad\n",
        "#optimizer.zero_grad() clears x.grad for every parameter x in the optimizer. It’s important to call this before loss.backward(), otherwise you’ll accumulate the gradients from multiple passes.#\n",
        "\n",
        "# Adam optimizer is used: both momentum and adaptive learning rate are used for better convergence.\n",
        "# In this, the model parameters are altered after computation of loss on each training example. \n",
        "#So, if the dataset contains 1000 rows SGD will update the model parameters 1000 times in one cycle of dataset instead of one time as in Gradient Descent.\n",
        "  #θ=θ−α⋅∇J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples.\n",
        "\n",
        "\n",
        "#All optimizers implement a step() method, that updates the parameters. It can be used in two ways:\n",
        "  #optimizer.step()\n",
        "#This is a simplified version supported by most optimizers. \n",
        "#The function can be called once the gradients are computed using e.g. backward().\n",
        "#lr_scheduler sets the learning rate of all parameter groups\n",
        "\n",
        "%timeit model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        outputs = model(**batch)\n",
        "        loss = outputs.loss #.loss calculates loss of output with given loss function\n",
        "        loss.backward() #basically does backpropagation. It calculates the gradient of the error function in a NN for each paramter x\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step() #calculated from optimizer\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2fQBJ1iHzhM"
      },
      "source": [
        "\n",
        "#to check the resulst we just use a metric from datasets library\n",
        "\n",
        "metric= load_metric(\"accuracy\")\n",
        "model.eval()\n",
        "for batch in eval_dataloader:\n",
        "    batch = {k: v.to(device) for k, v in batch.items()}\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**batch)\n",
        "\n",
        "    logits = outputs.logits\n",
        "    predictions = torch.argmax(logits, dim=-1)\n",
        "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
        "\n",
        "metric.compute()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}